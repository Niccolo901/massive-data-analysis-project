AMDA-Specific Concepts to Emphasize
1. Data Preprocessing at Scale
Syllabus Link: Technical preliminaries, HDFS, Spark

Use streaming-style data pipelines (tf.data, ImageDataGenerator, etc.)

Avoid loading all images at once — instead, use generators or batched datasets.

Show that your preprocessing can scale to millions of images if needed.

2. Efficient Dataset Handling
Syllabus Link: MapReduce, Spark

Though you’ll run the project on a smaller dataset, comment on how image URLs could be downloaded and processed in parallel, akin to Map tasks.

Optionally simulate a MapReduce-like approach:

Map = download and preprocess image

Reduce = aggregate into dataset

3. Modeling and Hyperparameter Selection
Syllabus Link: Recommendation systems, frequent itemsets (indirectly), and analytics

Apply grid/random search efficiently.

Comment on scalability of training: what happens as the image set grows?

If using a pre-trained model (e.g., ResNet, MobileNet), analyze:

Feature extraction time

Memory usage

Batch size vs. training speed tradeoff

4. Scalability Analysis
Syllabus Link: Non-functional performance & scalability, efficient algorithms
Your report should answer:

How does training time scale with dataset size?

Could this be adapted to distributed training (e.g., tf.distribute)?

Can feature extraction be parallelized?

5. Evaluation at Scale
Syllabus Link: Link Analysis, Similar Items
(Optional but advanced): Use cosine similarity or embedding-based distance between covers to analyze misclassifications or suggest related covers.



Integrate tf.data for Preprocessing: Use TensorFlow's tf.data to preprocess images in a scalable way.


Improve Parallelism: Use a distributed framework like Dask or Spark for true MapReduce-style processing.


Add Metrics and Logging: Track performance metrics (e.g., time per chunk, memory usage) to demonstrate scalability.


Handle Large Datasets: Write intermediate results to disk or a database to avoid memory bottlenecks.
